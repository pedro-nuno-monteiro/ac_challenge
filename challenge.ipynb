{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Import de todas as bibliotecas necessárias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Leitura do local dataframe + informações básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('RTA Dataset.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df_1 = df.copy()\n",
    "df_2 = df.copy()\n",
    "df_3 = df.copy()\n",
    "df_4 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()   #verificar quantas colunas têm valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - pre processing - divisão de experiências\n",
    "\n",
    " *  exp 1 - distribuir a proporção dos valores nulos (>4000) pelos restantes valores, mantendo a proporção\n",
    " *  exp 2 - eliminar colunas com valores nulos > 3500\n",
    " *  exp 3 - substituir valores nulos por valores em que a coluna da gravidade é a mesma (tendo em conta a sua moda)\n",
    " *  exp 4 - aplicar método do KNN às colunas com valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de repartir em experiências, vamos analisar o que significam algumas colunas que aparentam não ser tão importantes para a nossa análise, eliminando-as de qualquer experiência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_vehicle = df['Type_of_vehicle'].value_counts()    #ELIMINAR COM CERTEZA -> irrelevante\n",
    "print(type_of_vehicle, \"\\n\\n\")\n",
    "\n",
    "owner_of_vehicle = df['Owner_of_vehicle'].value_counts()  #ELIMINAR COM CERTEZA -> irrelevante\n",
    "print(owner_of_vehicle, \"\\n\\n\")\n",
    "\n",
    "service_year_of_vehicle = df['Service_year_of_vehicle'].value_counts()    #ELIMINAR COM CERTEZA -> muitos nans\n",
    "print(service_year_of_vehicle, \"\\n\\n\")\n",
    "\n",
    "defect_of_vehicle = df['Defect_of_vehicle'].value_counts()  #tem 7777 \"no defect\"\n",
    "print(defect_of_vehicle, \"\\n\\n\")    #ELIMINAR -> DADOS CONFUSOS\n",
    "\n",
    "area_accident_occured = df['Area_accident_occured'].value_counts() #ELIMINAR -> IRRELEVANTE\n",
    "print(area_accident_occured, \"\\n\\n\")\n",
    "\n",
    "road_allignment = df['Road_allignment'].value_counts()  #ELIMINAR -> muitos dados iguais -> +80%  \n",
    "print(road_allignment, \"\\n\\n\")\n",
    "\n",
    "pedestrian_movement = df['Pedestrian_movement'].value_counts()  #ELIMINAR -> dados sem sentido\n",
    "print(pedestrian_movement, \"\\n\\n\")\n",
    "\n",
    "fitness_of_casuality = df['Fitness_of_casuality'].value_counts()  #ELIMINAR -> faltam dados e muitos iguais\n",
    "print(fitness_of_casuality, \"\\n\\n\")\n",
    "\n",
    "work_of_causalty = df['Work_of_casuality'].value_counts()\n",
    "print(work_of_causalty, \"\\n\\n\")\n",
    "\n",
    "casualty_class = df['Casualty_class'].value_counts()  #ELIMINAR -> ver matriz de correlação\n",
    "print(casualty_class, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a análise, eliminamos as colunas que afetam pouco a nossa saída, que, como sabemos, é a última coluna: \"Gravidade do Acidente\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_a_eliminar = ['Type_of_vehicle', 'Owner_of_vehicle', 'Defect_of_vehicle', 'Area_accident_occured',\n",
    "                      'Road_allignment', 'Pedestrian_movement', 'Fitness_of_casuality', 'Work_of_casuality', 'Time', 'Day_of_week']\n",
    "\n",
    "df.drop(colunas_a_eliminar, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, para uma melhor compreensão da tabela, vamos renomear as colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time, day_of_week, age_band_of_driver, sex_of_driver, educational_level, vehicle_driver_relation, driving_experience\n",
    "#type_of_vehicle, owner_of_vehicle, service_year_of_vehicle, defect_of_vehicle, area_accident_occured\n",
    "#lanes_or_medians, road_allignment, types_of_junction, road_surface_type, road_surface_conditions, ligh_conditions\n",
    "#weather_conditions, type_of_collision, number_of_vehicles_involved, number_of_casualties, vehicle_movement\n",
    "#casualty_class, sex_of_casualty, age_band_of_casualty, casualty_severity, work_of_casualty, fitness_of_casualty\n",
    "#pedestrian_movement, cause_of_accident, accident_severity,\n",
    "colunas_renomeadas = ['Faixa Etária', 'Género', 'Nível de Educação', 'Relação com o Veículo', \n",
    "                      'Experiência de Condução', 'Idade do Veículo', 'Situação de Faixa', 'Tipo de Cruzamento',\n",
    "                      'Tipo de Estrada', 'Condições do Piso', 'Condições de Visibilidade',\n",
    "                      'Condiçoes Meteorológicas', 'Tipo de Colisão', 'N.º Veículos Envolvidos',\n",
    "                      'Número de Vítimas', 'Movimento do Veículo', 'Tipo de Vítima','Género da Vítima',\n",
    "                      'Faixa Etária da Vítima', 'Gravidade da Vítima',\n",
    "                      'Causa do Acidente', 'Gravidade do Acidente']\n",
    "\n",
    "df.columns = colunas_renomeadas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora perceber quantos valores nulos existem e ainda realizar a substituição desses mesmos pela moda da coluna.\n",
    "\n",
    "Neste passo, fomos apenar substituir os valores nulos cujas colunas tinham menos de 1000 valores \"NaN\", uma vez que afeta pouco a proporção dos valores, servindo de base a todas as experiências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('na', pd.NA, inplace=True) # substituir \"na\" por um valor realmente nulo\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Nível de Educação'] = df['Nível de Educação'].fillna(df['Nível de Educação'].mode()[0])\n",
    "df['Relação com o Veículo'] = df['Relação com o Veículo'].fillna(df['Relação com o Veículo'].mode()[0])\n",
    "df['Experiência de Condução'] = df['Experiência de Condução'].fillna(df['Experiência de Condução'].mode()[0])\n",
    "df['Situação de Faixa'] = df['Situação de Faixa'].fillna(df['Situação de Faixa'].mode()[0])\n",
    "df['Tipo de Cruzamento'] = df['Tipo de Cruzamento'].fillna(df['Tipo de Cruzamento'].mode()[0])\n",
    "df['Tipo de Estrada'] = df['Tipo de Estrada'].fillna(df['Tipo de Estrada'].mode()[0])\n",
    "df['Tipo de Colisão'] = df['Tipo de Colisão'].fillna(df['Tipo de Colisão'].mode()[0])\n",
    "df['Movimento do Veículo'] = df['Movimento do Veículo'].fillna(df['Movimento do Veículo'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.copy()\n",
    "df_2 = df.copy()\n",
    "df_3 = df.copy()\n",
    "df_4 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.1 - distribuir a proporção dos valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos as colunas que queremos alterar\n",
    "colunas_a_substituir = ['Idade do Veículo', 'Tipo de Vítima', 'Género da Vítima', 'Faixa Etária da Vítima', 'Gravidade da Vítima']\n",
    "\n",
    "for coluna in colunas_a_substituir:\n",
    "\n",
    "    no_valores_nulos = df_1[coluna].isnull().sum()\n",
    "\n",
    "    # proporção de valores não nulos para cada categoria\n",
    "    proporcoes_categoria = df_1[coluna].value_counts(normalize = True)\n",
    "\n",
    "    # calcular o número de valores nulos a serem distribuídos para cada categoria\n",
    "    distribuicao_nulos = (proporcoes_categoria * no_valores_nulos).round().astype(int)\n",
    "\n",
    "    # neste caso, damos assign a cada valor nulo de forma RANDOM\n",
    "    indices_nulo = df_1[df_1[coluna].isnull()].index\n",
    "    for category, count in distribuicao_nulos.items():\n",
    "        sample_indices = np.random.choice(indices_nulo, size = count, replace = False)\n",
    "        df_1.loc[sample_indices, coluna] = category\n",
    "\n",
    "    # preencher algum valor nulo que falte, RANDOM\n",
    "    remaining_null_indices = df_1[df_1[coluna].isnull()].index\n",
    "    remaining_categories = list(proporcoes_categoria.index)\n",
    "    for index in remaining_null_indices:\n",
    "        df_1.at[index, coluna] = np.random.choice(remaining_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_values = {}\n",
    "for column in df_1.columns:\n",
    "    counts = df_1[column].value_counts(normalize = True, dropna = False) * 100\n",
    "    percentage_values[column] = counts\n",
    "\n",
    "for column, percentages in percentage_values.items():\n",
    "    print(percentages)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.2 - eliminar colunas com valores nulos > 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_valores_nulos = ['Idade do Veículo', 'Tipo de Vítima', 'Género da Vítima', 'Faixa Etária da Vítima', 'Gravidade da Vítima']\n",
    "\n",
    "df_2.drop(colunas_valores_nulos, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.3 - substituir valores nulos por valores em que a coluna da gravidade é a mesma (tendo em conta a sua moda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_values = {}\n",
    "for column in colunas_a_substituir:\n",
    "    counts = df_3[column].value_counts(normalize = True, dropna = False) * 100\n",
    "    percentage_values[column] = counts\n",
    "\n",
    "for column, percentages in percentage_values.items():\n",
    "    print(percentages)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target column\n",
    "target_column = 'Gravidade do Acidente'\n",
    "\n",
    "# Iterate over each column with null values\n",
    "for column in colunas_a_substituir:\n",
    "    \n",
    "    # Calculate the proportions of each value in the column for each category in the target column\n",
    "    proportions = df_3.groupby(target_column)[column].value_counts(normalize=True)\n",
    "    \n",
    "    # For each null value in the column, replace it based on the proportions\n",
    "    for index, row in df_3[df_3[column].isnull()].iterrows():\n",
    "        \n",
    "        # Get the proportions for the target value of this row\n",
    "        target_value = row[target_column]\n",
    "        target_proportions = proportions[target_value]\n",
    "        \n",
    "        # Sample from the proportions to replace the null value\n",
    "        new_value = target_proportions.sample(weights=target_proportions).index[0]\n",
    "        \n",
    "        # Replace the null value\n",
    "        df_3.at[index, column] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_values = {}\n",
    "for column in colunas_a_substituir:\n",
    "    counts = df_3[column].value_counts(normalize = True, dropna = False) * 100\n",
    "    percentage_values[column] = counts\n",
    "\n",
    "for column, percentages in percentage_values.items():\n",
    "    print(percentages)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.4 - aplicar método do KNN às colunas com valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Initialize KNNImputer\n",
    "imputer = KNNImputer()\n",
    "\n",
    "# Get the columns with null values\n",
    "columns_with_null = df_4.columns[df_4.isnull().any()]\n",
    "\n",
    "# Iterate over each column with null values\n",
    "for column in columns_with_null:\n",
    "    # If the column is categorical, encode it using one-hot encoding or ordinal encoding\n",
    "    if df_4[column].dtype == 'object':\n",
    "        # Convert 'NAType' to NaN\n",
    "        df_4[column] = df_4[column].replace('NAType', np.nan)\n",
    "        # Handle missing values with a placeholder\n",
    "        df_4[column] = df_4[column].fillna('missing')\n",
    "        \n",
    "        # Apply ordinal encoding\n",
    "        encoder = OrdinalEncoder()\n",
    "        encoded_values = encoder.fit_transform(df_4[[column]])\n",
    "        encoded_df = pd.DataFrame(encoded_values, columns=[column+'_encoded'], index=df_4.index)\n",
    "        # Drop the original categorical column and concatenate the encoded columns\n",
    "        df_4 = pd.concat([df_4.drop(column, axis=1), encoded_df], axis=1)\n",
    "    else:\n",
    "        # Extract the column data\n",
    "        X = df_4.dropna(subset=[column], axis=0).drop(columns_with_null, axis=1)\n",
    "        y = df_4.dropna(subset=[column], axis=0)[column]\n",
    "        X_with_null = df_4[df_4[column].isnull()].drop(columns_with_null, axis=1)\n",
    "        \n",
    "        # Apply KNN imputation\n",
    "        imputer.fit(X, y)\n",
    "        imputed_values = imputer.transform(X_with_null)\n",
    "        \n",
    "        # Update the dataframe with imputed values\n",
    "        df_4.loc[X_with_null.index, column] = imputed_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Faixa Etária                      0\n",
       "Género                            0\n",
       "Nível de Educação                 0\n",
       "Relação com o Veículo             0\n",
       "Experiência de Condução           0\n",
       "Situação de Faixa                 0\n",
       "Tipo de Cruzamento                0\n",
       "Tipo de Estrada                   0\n",
       "Condições do Piso                 0\n",
       "Condições de Visibilidade         0\n",
       "Condiçoes Meteorológicas          0\n",
       "Tipo de Colisão                   0\n",
       "N.º Veículos Envolvidos           0\n",
       "Número de Vítimas                 0\n",
       "Movimento do Veículo              0\n",
       "Causa do Acidente                 0\n",
       "Gravidade do Acidente             0\n",
       "Idade do Veículo_1-2yr            0\n",
       "Idade do Veículo_2-5yrs           0\n",
       "Idade do Veículo_5-10yrs          0\n",
       "Idade do Veículo_Above 10yr       0\n",
       "Idade do Veículo_Below 1yr        0\n",
       "Idade do Veículo_Unknown          0\n",
       "Idade do Veículo_nan              0\n",
       "Tipo de Vítima_encoded            0\n",
       "Género da Vítima_encoded          0\n",
       "Faixa Etária da Vítima_encoded    0\n",
       "Gravidade da Vítima_encoded       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faixa Etária\n",
      "18-30       34.678467\n",
      "31-50       33.184475\n",
      "Over 51     12.869438\n",
      "Unknown     12.569016\n",
      "Under 18     6.698603\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Género\n",
      "Male       92.862943\n",
      "Female      5.691783\n",
      "Unknown     1.445274\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Nível de Educação\n",
      "Junior high school    67.879182\n",
      "Elementary school     17.562520\n",
      "High school            9.012666\n",
      "Above high school      2.939266\n",
      "Writing & reading      1.429035\n",
      "Unknown                0.811952\n",
      "Illiterate             0.365378\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Relação com o Veículo\n",
      "Employee    82.867814\n",
      "Owner       16.019812\n",
      "Other        0.998701\n",
      "Unknown      0.113673\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Experiência de Condução\n",
      "5-10yr        34.037025\n",
      "2-5yr         21.216304\n",
      "Above 10yr    18.366353\n",
      "1-2yr         14.257876\n",
      "Below 1yr     10.896395\n",
      "No Licence     0.958103\n",
      "unknown        0.267944\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Situação de Faixa\n",
      "Two-way (divided with broken lines road marking)    38.941215\n",
      "Undivided Two way                                   30.821695\n",
      "other                                               13.478402\n",
      "Double carriageway (median)                          8.281910\n",
      "One way                                              6.860994\n",
      "Two-way (divided with solid lines road marking)      1.152972\n",
      "Unknown                                              0.462813\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Tipo de Cruzamento\n",
      "Y Shape        44.088990\n",
      "No junction    31.154596\n",
      "Crossing       17.676194\n",
      "Other           3.613186\n",
      "Unknown         1.550828\n",
      "O Shape         1.331601\n",
      "T Shape         0.487171\n",
      "X Shape         0.097434\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Tipo de Estrada\n",
      "Asphalt roads                       93.114648\n",
      "Earth roads                          2.906788\n",
      "Gravel roads                         1.964924\n",
      "Other                                1.355960\n",
      "Asphalt roads with some distress     0.657681\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Condições do Piso\n",
      "Dry                     75.836310\n",
      "Wet or damp             23.579084\n",
      "Snow                     0.568366\n",
      "Flood over 3cm. deep     0.016239\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Condições de Visibilidade\n",
      "Daylight                   71.435531\n",
      "Darkness - lights lit      26.680741\n",
      "Darkness - no lighting      1.558948\n",
      "Darkness - lights unlit     0.324781\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Condiçoes Meteorológicas\n",
      "Normal               81.706723\n",
      "Raining              10.807080\n",
      "Other                 2.403378\n",
      "Unknown               2.370900\n",
      "Cloudy                1.014940\n",
      "Windy                 0.795713\n",
      "Snow                  0.495291\n",
      "Raining and Windy     0.324781\n",
      "Fog or mist           0.081195\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Tipo de Colisão\n",
      "Vehicle with vehicle collision             72.499188\n",
      "Collision with roadside objects            14.501462\n",
      "Collision with pedestrians                  7.275089\n",
      "Rollover                                    3.223449\n",
      "Collision with animals                      1.388438\n",
      "Collision with roadside-parked vehicles     0.438454\n",
      "Fall from vehicles                          0.276064\n",
      "Other                                       0.211108\n",
      "Unknown                                     0.113673\n",
      "With Train                                  0.073076\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "N.º Veículos Envolvidos\n",
      "2    67.716791\n",
      "1    16.206561\n",
      "3    12.731406\n",
      "4     2.947386\n",
      "6     0.341020\n",
      "7     0.056837\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Número de Vítimas\n",
      "1    68.179604\n",
      "2    18.593699\n",
      "3     7.380643\n",
      "4     3.199091\n",
      "5     1.680741\n",
      "6     0.722637\n",
      "7     0.178629\n",
      "8     0.064956\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Movimento do Veículo\n",
      "Going straight         68.739851\n",
      "Moving Backward         7.997727\n",
      "Other                   7.607990\n",
      "Reversing               4.571289\n",
      "Turnover                3.970445\n",
      "Getting off             2.752517\n",
      "Entering a junction     1.567067\n",
      "Overtaking              0.779474\n",
      "Unknown                 0.714518\n",
      "Stopping                0.495291\n",
      "U-Turn                  0.405976\n",
      "Waiting to go           0.316661\n",
      "Parked                  0.081195\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Causa do Acidente\n",
      "No distancing                           18.374472\n",
      "Changing lane to the right              14.680091\n",
      "Changing lane to the left               11.960052\n",
      "Driving carelessly                      11.383566\n",
      "No priority to vehicle                   9.800260\n",
      "Moving Backward                          9.231893\n",
      "No priority to pedestrian                5.854173\n",
      "Other                                    3.702501\n",
      "Overtaking                               3.491393\n",
      "Driving under the influence of drugs     2.760637\n",
      "Driving to the left                      2.305943\n",
      "Getting off the vehicle improperly       1.599545\n",
      "Driving at high speed                    1.412796\n",
      "Overturning                              1.209808\n",
      "Turnover                                 0.633323\n",
      "Overspeed                                0.495291\n",
      "Overloading                              0.479052\n",
      "Drunk driving                            0.219227\n",
      "Unknown                                  0.202988\n",
      "Improper parking                         0.202988\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Gravidade do Acidente\n",
      "Slight Injury     84.564794\n",
      "Serious Injury    14.152322\n",
      "Fatal injury       1.282884\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Idade do Veículo_1-2yr\n",
      "0.0    93.285158\n",
      "1.0     6.714842\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Idade do Veículo_2-5yrs\n",
      "0.0    85.449821\n",
      "1.0    14.550179\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Idade do Veículo_5-10yrs\n",
      "0.0    89.607015\n",
      "1.0    10.392985\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Idade do Veículo_Above 10yr\n",
      "0.0    89.249756\n",
      "1.0    10.750244\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Idade do Veículo_Below 1yr\n",
      "0.0    97.710296\n",
      "1.0     2.289704\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Idade do Veículo_Unknown\n",
      "0.0    76.591426\n",
      "1.0    23.408574\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Idade do Veículo_nan\n",
      "0.0    68.106528\n",
      "1.0    31.893472\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Tipo de Vítima_encoded\n",
      "0.0    40.142904\n",
      "3.0    36.075024\n",
      "2.0    13.389087\n",
      "1.0    10.392985\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Género da Vítima_encoded\n",
      "1.0    42.651835\n",
      "2.0    36.075024\n",
      "0.0    21.273141\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Faixa Etária da Vítima_encoded\n",
      "5.0    36.075024\n",
      "0.0    25.535888\n",
      "1.0    19.933420\n",
      "4.0     8.403703\n",
      "3.0     8.070802\n",
      "2.0     1.981163\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Gravidade da Vítima_encoded\n",
      "2.0    57.453719\n",
      "3.0    36.075024\n",
      "1.0     6.260149\n",
      "0.0     0.211108\n",
      "Name: proportion, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentage_values = {}\n",
    "for column in df_4.columns:\n",
    "    counts = df_4[column].value_counts(normalize = True, dropna = False) * 100\n",
    "    percentage_values[column] = counts\n",
    "\n",
    "for column, percentages in percentage_values.items():\n",
    "    print(percentages)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELIMINAR AS COLUNAS 4000 E TRAZER ALGUMAS DE VOLTA QUE ELIMINÁMOS MAIS CEDO\n",
    "\n",
    "# várias experiências (exp1 - manter proporção, exp2 - subs pela moda, exp3 - fazer o KNN, exp4 - eliminar colunas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao analisar as 4 colunas com mais de 1000 valores \"NA\", temos 2 soluções: ou eliminamos as colunas, ou repartimos a sua proporção pelos restantes valores da coluna.\n",
    "\n",
    "Eliminar uma coluna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartir a proporção pelos restantes valores das colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to process\n",
    "columns_to_process = ['Gravidade da Vítima', 'Faixa Etária da Vítima', 'Ocupação da Vítima']\n",
    "\n",
    "for column in columns_to_process:\n",
    "    # Number of null values in the column\n",
    "    null_count = df[column].isnull().sum()\n",
    "\n",
    "    # Proportion of non-null values for each category\n",
    "    category_proportions = df[column].value_counts(normalize=True)\n",
    "\n",
    "    # Calculate the number of null values to be distributed for each category\n",
    "    null_distribution = (category_proportions * null_count).round().astype(int)\n",
    "\n",
    "    # Randomly assign the null values to each category\n",
    "    null_indices = df[df[column].isnull()].index\n",
    "    for category, count in null_distribution.items():\n",
    "        sample_indices = np.random.choice(null_indices, size=count, replace=False)\n",
    "        df.loc[sample_indices, column] = category\n",
    "\n",
    "    # Fill any remaining null values randomly\n",
    "    remaining_null_indices = df[df[column].isnull()].index\n",
    "    remaining_categories = list(category_proportions.index)\n",
    "    for index in remaining_null_indices:\n",
    "        df.at[index, column] = np.random.choice(remaining_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Num ponto à parte, percebemos que eliminar as linhas com valores nulos não é uma opção, uma vez que perderíamos metade dos dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_linhas_na = df.isna().any(axis=1).sum()\n",
    "\n",
    "print(\"Total de linhas com valores NaN\", no_linhas_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 - Fazer matriz de correlação e importância de variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, vamos analisar a matriz de correlação entre as várias variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()  # aplicar Label Encoder a todas as colunas \"object\"\n",
    "for column in df_3.select_dtypes(include = 'object').columns:\n",
    "    df_3[column] = label_encoder.fit_transform(df_3[column])\n",
    "\n",
    "correlation_matrix = df_3.corr()\n",
    "plt.figure(figsize = (20, 15))\n",
    "sns.heatmap(correlation_matrix, annot = True, cmap = 'vlag', fmt = \".2f\", linewidths = 1, square=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df[['Gravidade da Vítima', 'Gravidade do Acidente']]\n",
    "correlation_matrix = df_2.corr()\n",
    "plt.figure(figsize = (10, 5))\n",
    "sns.heatmap(correlation_matrix, annot = True, cmap = 'vlag', fmt = \".4f\", linewidths = 1, square=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab(df['Gravidade do Acidente'], df['Faixa Etária da Vítima'], normalize='index') * 100\n",
    "\n",
    "# Display the proportions\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida, analisamos a importância de cada feature para o modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values # todas as features\n",
    "Y =  df.iloc[:, -1].values # feature target\n",
    "\n",
    "modelo = ExtraTreesClassifier()\n",
    "modelo.fit(X, Y)\n",
    "\n",
    "feature_importances = pd.Series(modelo.feature_importances_, index = df.columns[:-1])\n",
    "feature_importances.nlargest(10).plot(kind='barh')  # mostrar as 10 features mais importantes\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste ponto, dividimos os dados em sets de treino e de teste, utilizando 25% dos dados para teste e os restantes para teino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 30)\n",
    "\n",
    "print(\"Número de exemplos nos dados de treino: \", X_train.shape[0])\n",
    "print(\"Número de exemplos nos dados de teste: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 6 - Árvores de Decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro modelo de machine learning utilizado foi o modelo de árvores de decisão.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro vamos estanderizar as nossas features. Isso é feito para garantir que todas as características tenham a mesma escala, o que pode melhorar o desempenho do algoritmo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O parâmetro max_depth = 4 define a profundidade máxima da árvore de decisão, \n",
    "# visto que limitar a profundidade pode ajudar a evitar overfitting, tornando a árvore mais simples.\n",
    "clf = DecisionTreeClassifier(random_state = 42, criterion = \"entropy\", max_depth = 4)\n",
    "\n",
    "# Treino: Arvore de Decisão\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Previsão da resposta --> Teste\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "\n",
    "# Print dos valores accuracy\n",
    "print('Train data accuracy: ', accuracy_score(y_true = y_train, y_pred = y_train_pred))\n",
    "print('Test data accuracy: ', accuracy_score(y_true = y_test, y_pred = y_test_pred))\n",
    "print('Decision tree accuracy: ', accuracy_score(y_test_pred, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "tree.plot_tree(clf, fontsize = 8, feature_names = df.columns[:-1], filled = True, rounded = True, proportion=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Através do classification report, podemos concluir a acurácia do modelo tal como o recall, a precisão e o f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred, zero_division = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após realizar um classification report, vamos agora mostrar a matriz de confusão. Assim podemos ver a quantidade de falsos positivos e falsos negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot = True, cmap = 'seismic', fmt = \"4.0f\")\n",
    "ax.set_title('Seaborn Confusion Matrix\\n\\n')\n",
    "ax.set_xlabel('\\nPredicted decision case disposition')\n",
    "ax.set_ylabel('Actual decision case disposition ')\n",
    "\n",
    "print(cf_matrix)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
